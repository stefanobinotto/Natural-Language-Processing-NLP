# Neural Dependency Parser
This notebook contains my final project for the **Natural Language Processing** course (DEI UniPD - academic year 2023/24) consisting in a **neural transition-based parsing for dependency grammars with unlabelled dependencies**. The **arc-eager parsing algorithm** and the static oracle traditionally used for this parser are described in [Section 2](https://aclanthology.org/C12-1059/).

We were required to create a baseline model which used biLSTM for extracting features from the input words. Then we needed to develop a second model using **BERT** in place of the **biLSTM**. Since BERT assigns embeddings to each token derived using BPE, in order to derive embeddings for words that are split into several tokens, I just took the embedding of the left-most token for the word. I used Hugging Face library for the implementation and the training of biLSTM and for the implementation and the fine-tuning of BERT.

For training I used the dependency treebank **UD English EWT** ([link here](https://universaldependencies.org/treebanks/en_ewt/index.html)), a Gold Standard Universal Dependencies Corpus for English.

This project was presented as a notebook containing the following sections.

1. Dataset analysis and report of any useful information

2. Description of baseline model and BERT-based model.

3. Data set-up and training.

4. Evaluation: using unlabeled attachment score (UAS), and drawing a comparison between the two models and perform some error analysis for the BERT-based model.

5. Brief discussion of SotA for this task.
